{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Errors\n",
    "\n",
    "Here are two datasets that represent two of the examples you have seen in this lesson.  \n",
    "\n",
    "One dataset is based on the parachute example, and the second is based on the judicial example.  Neither of these datasets are based on real people.\n",
    "\n",
    "Use the questions below to assist in answering the quiz questions at the bottom of this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "jud_data = pd.read_csv('data/judicial_dataset_predictions.csv')\n",
    "par_data = pd.read_csv('data/parachute_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>defendant_id</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22574</td>\n",
       "      <td>innocent</td>\n",
       "      <td>innocent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35637</td>\n",
       "      <td>innocent</td>\n",
       "      <td>innocent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39919</td>\n",
       "      <td>innocent</td>\n",
       "      <td>innocent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29610</td>\n",
       "      <td>guilty</td>\n",
       "      <td>guilty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38273</td>\n",
       "      <td>innocent</td>\n",
       "      <td>innocent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   defendant_id    actual predicted\n",
       "0         22574  innocent  innocent\n",
       "1         35637  innocent  innocent\n",
       "2         39919  innocent  innocent\n",
       "3         29610    guilty    guilty\n",
       "4         38273  innocent  innocent"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jud_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7283 entries, 0 to 7282\n",
      "Data columns (total 3 columns):\n",
      "defendant_id    7283 non-null int64\n",
      "actual          7283 non-null object\n",
      "predicted       7283 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 170.8+ KB\n"
     ]
    }
   ],
   "source": [
    "jud_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Judicial dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Above, you can see the actual and predicted columns for each of the datasets.  Using the **jud_data**, find the proportion of errors for the dataset, and furthermore, the percentage of errors of each type.  Use the results to answer the questions in quiz 1 below.  \n",
    "\n",
    "**Hint for quiz:** an error is any time the prediction doesn't match an actual value.  Additionally, there are Type I and Type II errors to think about.  We also know we can minimize one type of error by maximizing the other type of error.  If we predict all individuals as innocent, how many of the guilty are incorrectly labeled?  Similarly, if we predict all individuals as guilty, how many of the innocent are incorrectly labeled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error 0.042152958945489497\n",
      "Type one error 0.001510366607167376\n",
      "Type two error 0.04064259233832212\n"
     ]
    }
   ],
   "source": [
    "# Total error\n",
    "error_total = (jud_data['actual'] != jud_data['predicted']).sum() / len(jud_data)\n",
    "error_total\n",
    "\n",
    "# Type one error: Percentage of innocent people predicted to be guilty\n",
    "error1 = ((jud_data['actual'] == 'innocent') & (jud_data['predicted'] == 'guilty')).sum() / len(jud_data)\n",
    "\n",
    "\n",
    "# Type two error: Percentage of guilty people predicted to be innocent\n",
    "error2 = ((jud_data['actual'] == 'guilty') & (jud_data['predicted'] == 'innocent')).sum() / len(jud_data)\n",
    "\n",
    "print('Total error', error_total)\n",
    "print('Type one error', error1)\n",
    "print('Type two error', error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict all indivisuals as innocent: Type two error\n",
    "# We don't have to calculate. If Predict all indivisuals as guilty, type two errors must be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45159961554304545"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict all indivisuals as innocent: Type one error\n",
    "error4 = (jud_data['actual'] == 'innocent').sum() / len(jud_data)\n",
    "error4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Using the **par_data**, find the proportion of errors for the dataset, and furthermore, the percentage of errors of each type.  Use the results to answer the questions in quiz 2 below.\n",
    "\n",
    "These should be very similar operations to those you performed in the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Parahcute dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parachute_id</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4038</th>\n",
       "      <td>8107</td>\n",
       "      <td>opens</td>\n",
       "      <td>opens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4515</th>\n",
       "      <td>4601</td>\n",
       "      <td>opens</td>\n",
       "      <td>opens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>4336</td>\n",
       "      <td>opens</td>\n",
       "      <td>fails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>3739</td>\n",
       "      <td>opens</td>\n",
       "      <td>opens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3787</th>\n",
       "      <td>1226</td>\n",
       "      <td>opens</td>\n",
       "      <td>opens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      parachute_id actual predicted\n",
       "4038          8107  opens     opens\n",
       "4515          4601  opens     opens\n",
       "1855          4336  opens     fails\n",
       "2156          3739  opens     opens\n",
       "3787          1226  opens     opens"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5829 entries, 0 to 5828\n",
      "Data columns (total 3 columns):\n",
      "parachute_id    5829 non-null int64\n",
      "actual          5829 non-null object\n",
      "predicted       5829 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 136.7+ KB\n"
     ]
    }
   ],
   "source": [
    "par_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error 0.039972551037913875\n",
      "Type one error 0.00017155601303825698\n",
      "Type two error 0.03980099502487562\n"
     ]
    }
   ],
   "source": [
    "# Total error\n",
    "error_total = (par_data['actual'] != par_data['predicted']).sum() / len(par_data)\n",
    "error_total\n",
    "\n",
    "# Type one error: Percentage of innocent people predicted to be guilty\n",
    "error1 = ((par_data['actual'] == 'fails') & (par_data['predicted'] == 'opens')).sum() / len(par_data)\n",
    "\n",
    "\n",
    "# Type two error: Percentage of guilty people predicted to be innocent\n",
    "error2 = ((par_data['actual'] == 'opens') & (par_data['predicted'] == 'fails')).sum() / len(par_data)\n",
    "\n",
    "print('Total error', error_total)\n",
    "print('Type one error', error1)\n",
    "print('Type two error', error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9917653113741637"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error4 = (par_data['actual'] == 'opens').sum() / len(par_data)\n",
    "error4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
